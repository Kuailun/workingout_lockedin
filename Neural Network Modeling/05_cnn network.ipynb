{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Global Settings",
   "id": "8bc32bc233a1a43b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ------------------------\n",
    "# define global settings\n",
    "# ------------------------\n",
    "SETTING_PIPELINE_NAME = \"140_clean version\"\n",
    "\n",
    "# ------------------------\n",
    "# import packages\n",
    "# ------------------------\n",
    "import os\n",
    "\n",
    "DIR_HOME = \"XXXXXX\"\n",
    "DIR_CURRENT = os.getcwd()\n",
    "DIR_PIPELINE = os.path.join(DIR_HOME, \"2_pipeline\", SETTING_PIPELINE_NAME)\n",
    "if not os.path.exists(DIR_PIPELINE):\n",
    "    os.mkdir(DIR_PIPELINE)\n",
    "    os.mkdir(os.path.join(DIR_PIPELINE, \"out\"))\n",
    "    os.mkdir(os.path.join(DIR_PIPELINE, \"store\"))\n",
    "    os.mkdir(os.path.join(DIR_PIPELINE, \"temp\"))"
   ],
   "id": "d022c7ac843f97ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Folders\n",
    "csv_dir = \"XXXXXX\"\n",
    "csv_files = [os.path.join(csv_dir, f) for f in os.listdir(csv_dir) if f.endswith('.csv')]\n",
    "\n",
    "# Time\n",
    "ACTIVE_DURATION_THRESHOLD = 150 # minutes\n",
    "normal_test_point = 44\n",
    "covid_train_point = 54\n",
    "covid_test_point = 118\n",
    "\n",
    "# Columns\n",
    "# x_cols = ['user_gender', 'register_index', 'TEMP', 'TEMP2', 'WIND', 'HUMI', 'HUMI2', 'VISI', 'PRES', 'CLOD', 'PRCP']\n",
    "# x_cols = ['user_gender', 'register_index', 'TEMP', 'TEMP2', 'WIND', 'HUMI', 'HUMI2', 'VISI', 'PRES', 'CLOD', 'PRCP', 'IS_COVID_START', 'POLICY']\n",
    "# x_cols = ['user_gender', 'register_index', 'TEMP', 'TEMP2', 'WIND', 'HUMI', 'HUMI2', 'VISI', 'PRES', 'CLOD', 'PRCP', 'IS_COVID_START', 'POLICY','DURATION_1', 'DURATION_2', 'DURATION_3']\n",
    "x_cols = ['user_gender', 'register_index', 'TEMP', 'TEMP2', 'WIND', 'HUMI', 'HUMI2', 'VISI', 'PRES', 'CLOD', 'PRCP', 'IS_COVID_START', 'POLICY','DURATION_1', 'DURATION_2', 'DURATION_3', 'LG_FOLLOW']\n",
    "y_col = 'DURATION_0'\n",
    "\n",
    "# Train\n",
    "BATCH_SIZE = 1024\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')"
   ],
   "id": "2ab4e294cd221a6c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Dataset Function",
   "id": "6143434149524ef4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the conditions as functions\n",
    "def training_condition(idx):\n",
    "    return idx < 3 + normal_test_point or 3 + covid_train_point <= idx < 3 + covid_test_point\n",
    "\n",
    "def testing_normal_condition(idx):\n",
    "    return 3 + normal_test_point <= idx < 3 + covid_train_point\n",
    "\n",
    "def testing_covid_condition(idx):\n",
    "    return idx >= 3 + covid_test_point"
   ],
   "id": "304cdbd56856f6f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "feature_mean = {'TEMP': 14.007015458415168, 'TEMP2': 279.26179025763645, 'WIND': 11.920575962103781, 'HUMI': 70.86096441758492, 'HUMI2': 5283.037685289832, 'VISI': 8.704992884116393, 'PRES': 1014.7379409770316, 'CLOD': 43.15788405638498, 'PRCP': 24.918650844505734, 'DURATION_0': 102.90183371116115, 'DURATION_1': 102.90183371116115, 'DURATION_2': 102.90183371116115,'DURATION_3': 102.90183371116115}\n",
    "feature_std = {'TEMP': 8.328565701080262, 'TEMP2': 250.7086255558244, 'WIND': 5.50769354766836, 'HUMI': 13.86840936112677, 'HUMI2': 1766.4921236773441, 'VISI': 0.8701007169616094, 'PRES': 6.589018889937119, 'CLOD': 22.034113767171334, 'PRCP': 34.64585327001472, 'DURATION_0': 156.30188955998761, 'DURATION_1': 156.30188955998761, 'DURATION_2': 156.30188955998761, 'DURATION_3': 156.30188955998761}\n",
    "\n",
    "THRESHOLD_0 = (0 - feature_mean['DURATION_0']) / feature_std['DURATION_0']\n",
    "THRESHOLD_150 = (150 - feature_mean['DURATION_0']) / feature_std['DURATION_0']"
   ],
   "id": "e27483fa0d84933b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Dataset type\n",
    "DT_TRAIN = 1\n",
    "DT_TEST_NORMAL = 2\n",
    "DT_TEST_COVID = 3"
   ],
   "id": "b928a8b28200a1d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class CSVDataset(Dataset):\n",
    "    def __init__(self, csv_data, x_cols, y_col, dataset_type):\n",
    "        self.x_cols = x_cols\n",
    "        self.y_col = y_col\n",
    "        self.csv_files = csv_files\n",
    "        \n",
    "        # Get right data type\n",
    "        if dataset_type == DT_TRAIN:\n",
    "            data = csv_data[csv_data[\"INDEX\"].apply(training_condition)]\n",
    "        elif dataset_type == DT_TEST_NORMAL:\n",
    "            data = csv_data[csv_data[\"INDEX\"].apply(testing_normal_condition)]\n",
    "        elif dataset_type == DT_TEST_COVID:\n",
    "            data = csv_data[csv_data[\"INDEX\"].apply(testing_covid_condition)]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid dataset type\")\n",
    "        \n",
    "        # standardize data\n",
    "        for feat in feature_mean:\n",
    "            data[feat] = (data[feat] - feature_mean[feat]) / feature_std[feat]\n",
    "        \n",
    "        # Extract features and labels\n",
    "        self.x = torch.tensor(data[self.x_cols].values, dtype=torch.float32)\n",
    "        self.y = torch.tensor(data[self.y_col].values.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "class CSVDatasetWeighted(Dataset):\n",
    "    def __init__(self, csv_data, x_cols, y_col, dataset_type):\n",
    "        self.x_cols = x_cols\n",
    "        self.y_col = y_col\n",
    "        self.csv_data = csv_data\n",
    "        \n",
    "        # Get right data type\n",
    "        if dataset_type == DT_TRAIN:\n",
    "            data = csv_data[csv_data[\"INDEX\"].apply(training_condition)]\n",
    "        elif dataset_type == DT_TEST_NORMAL:\n",
    "            data = csv_data[csv_data[\"INDEX\"].apply(testing_normal_condition)]\n",
    "        elif dataset_type == DT_TEST_COVID:\n",
    "            data = csv_data[csv_data[\"INDEX\"].apply(testing_covid_condition)]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid dataset type\")\n",
    "        \n",
    "        # standardize data\n",
    "        for feat in feature_mean:\n",
    "            data[feat] = (data[feat] - feature_mean[feat]) / feature_std[feat]\n",
    "\n",
    "        # Balance the dataset\n",
    "        active_records = data[data[self.y_col] >= THRESHOLD_150]\n",
    "        inactive_records = data[data[self.y_col] < THRESHOLD_150]\n",
    "        \n",
    "        if len(inactive_records) > len(active_records):\n",
    "            inactive_records = inactive_records.sample(len(active_records), random_state=42)\n",
    "        elif len(active_records) > len(inactive_records):\n",
    "            active_records = active_records.sample(len(inactive_records), random_state=42)\n",
    "\n",
    "        balanced_data = pd.concat([active_records, inactive_records])\n",
    "        \n",
    "        # Extract features and labels\n",
    "        self.x = torch.tensor(balanced_data[self.x_cols].values, dtype=torch.float32)\n",
    "        self.y = torch.tensor(balanced_data[self.y_col].values.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def compute_weights(y_train, threshold):\n",
    "    print(\"Debugging weights computation:\")\n",
    "    \n",
    "    # Ensure y_train is a tensor and convert to numpy array for computation\n",
    "    y_train = y_train.cpu().numpy().squeeze()\n",
    "    print(f\"y_train (after squeeze): {y_train.shape}\")\n",
    "    \n",
    "    # Calculate active and inactive masks\n",
    "    y_train_1 = (y_train >= threshold).astype(float)\n",
    "    y_train_0 = (y_train < threshold).astype(float)\n",
    "    print(f\"Number of active records: {sum(y_train_1)}, Number of inactive records: {sum(y_train_0)}\")\n",
    "    \n",
    "    # Calculate weights\n",
    "    total_records = sum(y_train_1) + sum(y_train_0)\n",
    "    weight_1 = sum(y_train_0) / total_records\n",
    "    weight_0 = sum(y_train_1) / total_records\n",
    "    print(f\"Weight active: {weight_0}\")\n",
    "    print(f\"Weight inactive: {weight_1}\")\n",
    "    \n",
    "    # Assign weights\n",
    "    y_train_weight = y_train_1 * weight_1 + y_train_0 * weight_0\n",
    "    \n",
    "    # Normalize weights\n",
    "    y_train_weight /= y_train_weight.sum()\n",
    "    \n",
    "    print(\"Sample weights tensor:\", y_train_weight[:10])  # Print first 10 weights for debugging\n",
    "    print(f\"Total weights sum: {y_train_weight.sum()} (should be close to 1.0)\")\n",
    "    \n",
    "    return y_train_weight"
   ],
   "id": "c613f49e22f251f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Read and combine all CSV files\n",
    "print(\"Reading csvs\")\n",
    "data = pd.concat([pd.read_csv(file) for file in tqdm(csv_files)], ignore_index=True)\n",
    "\n",
    "train_dataset = CSVDataset(data, x_cols, y_col, DT_TRAIN)\n",
    "test_normal_dataset = CSVDataset(data, x_cols, y_col, DT_TEST_NORMAL)\n",
    "test_covid_dataset = CSVDataset(data, x_cols, y_col, DT_TEST_COVID)\n",
    "train_dataset_weighted = CSVDatasetWeighted(data, x_cols, y_col, DT_TRAIN)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "test_normal_dataloader = DataLoader(test_normal_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "test_covid_dataloader = DataLoader(test_covid_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "train_dataloader_weighted = DataLoader(train_dataset_weighted, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "# weights = compute_weights(train_dataset.y, THRESHOLD_150)\n",
    "# # Create WeightedRandomSampler\n",
    "# sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n",
    "# # Create DataLoader\n",
    "# train_dataloader_weighted = DataLoader(train_dataset, sampler=sampler, batch_size=1024)\n",
    "\n",
    "# Debugging the dataloader\n",
    "for inputs, labels in train_dataloader:\n",
    "    print(f\"Batch inputs dtype: {inputs.dtype}, shape: {inputs.shape}, device: {inputs.device}\")\n",
    "    print(f\"Batch labels dtype: {labels.dtype}, shape: {labels.shape}, device: {labels.device}\")\n",
    "    break  # Just to print the first batch"
   ],
   "id": "c725144703f9c187",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Linear Regression",
   "id": "e2a0037d59430248"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score, f1_score\n",
    "\n",
    "def scores(y_true, y_pred, mode):\n",
    "    y_true = np.array(y_true, dtype=np.float64)\n",
    "    y_pred = np.array(y_pred, dtype=np.float64)\n",
    "    y_pred[y_pred<THRESHOLD_0] = THRESHOLD_0\n",
    "\n",
    "    y_under_150 = (y_true<THRESHOLD_150) * 1\n",
    "    y_above_150 = (y_true>=THRESHOLD_150) * 1\n",
    "\n",
    "    pred_under_150 = (y_pred<THRESHOLD_150) * 1\n",
    "    pred_above_150 = (y_pred>=THRESHOLD_150) * 1\n",
    "\n",
    "    y_0_pred_0 = int(sum(y_under_150 & pred_under_150))\n",
    "    y_0_pred_1 = int(sum(y_under_150 & pred_above_150))\n",
    "    y_1_pred_0 = int(sum(y_above_150 & pred_under_150))\n",
    "    y_1_pred_1 = int(sum(y_above_150 & pred_above_150))\n",
    "\n",
    "    total = len(y_true)\n",
    "    correct = int(y_0_pred_0) + int(y_1_pred_1)\n",
    "\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_above_150, pred_above_150)\n",
    "\n",
    "    ret = {\n",
    "        mode + ' y_0 pred_0': y_0_pred_0,\n",
    "        mode + ' y_0 pred_1': y_0_pred_1,\n",
    "        mode + ' y_1 pred_0': y_1_pred_0,\n",
    "        mode + ' y_1 pred_1': y_1_pred_1,\n",
    "        mode + ' lazy acc': y_0_pred_0 / (y_0_pred_0 + y_0_pred_1),\n",
    "        mode + ' active acc': y_1_pred_1 / (y_1_pred_0 + y_1_pred_1),\n",
    "        mode + ' r2_score': r2,\n",
    "        mode + ' f1_score': f1,\n",
    "        mode + ' mse': mean_squared_error(y_true, y_pred),\n",
    "        mode + ' acc': correct/total\n",
    "    }\n",
    "\n",
    "    return ret\n",
    "\n",
    "def evaluate_model(X_train, y_train, X_test_normal, y_test_normal, X_test_covid, y_test_covid,pmodel):\n",
    "    ret = {}\n",
    "\n",
    "    train_pred = pmodel.predict(X_train)\n",
    "    test_normal_pred = pmodel.predict(X_test_normal)\n",
    "    test_covid_pred = pmodel.predict(X_test_covid)\n",
    "\n",
    "    train_pred[train_pred<THRESHOLD_0] = THRESHOLD_0\n",
    "    test_normal_pred[test_normal_pred<THRESHOLD_0] = THRESHOLD_0\n",
    "    test_covid_pred[test_covid_pred<THRESHOLD_0] = THRESHOLD_0\n",
    "\n",
    "    res_train = scores(y_train, train_pred, 'Train')\n",
    "    res_test_normal = scores(y_test_normal, test_normal_pred, 'Test normal')\n",
    "    res_test_covid = scores(y_test_covid, test_covid_pred, 'Test covid')\n",
    "\n",
    "    for key in res_train:\n",
    "        ret[key] = res_train[key]\n",
    "\n",
    "    for key in res_test_normal:\n",
    "        ret[key] = res_test_normal[key]\n",
    "\n",
    "    for key in res_test_covid:\n",
    "        ret[key] = res_test_covid[key]\n",
    "\n",
    "    return ret"
   ],
   "id": "36e811f0238746f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pprint\n",
    "\n",
    "def regression_linear(X_train, y_train):\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "nn_train_x = train_dataset.x.cpu().numpy()\n",
    "nn_train_y = train_dataset.y.cpu().numpy()\n",
    "nn_test_normal_x = test_normal_dataset.x.cpu().numpy()\n",
    "nn_test_normal_y = test_normal_dataset.y.cpu().numpy()\n",
    "nn_test_covid_x = test_covid_dataset.x.cpu().numpy()\n",
    "nn_test_covid_y = test_covid_dataset.y.cpu().numpy()\n",
    "\n",
    "model = regression_linear(nn_train_x, nn_train_y)\n",
    "ret = evaluate_model(nn_train_x, nn_train_y, nn_test_normal_x, nn_test_normal_y, nn_test_covid_x, nn_test_covid_y, model)\n",
    "pprint.pprint(ret)"
   ],
   "id": "22d5d33993c61c51",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model",
   "id": "3c030dcd75bb9298"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class NN1(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NN1, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 16)\n",
    "        self.fc2 = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class NN2(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NN2, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 16)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.fc2 = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.silu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class NN3(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NN3, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 1)\n",
    "        self.silu1 = nn.SiLU()\n",
    "        self.silu2 = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.silu1(self.fc1(x))\n",
    "        x = self.silu2(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class NN4(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NN4, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 4)\n",
    "        self.fc4 = nn.Linear(4, 1)\n",
    "        self.silu1 = nn.SiLU()\n",
    "        self.silu2 = nn.SiLU()\n",
    "        self.silu3 = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.silu1(self.fc1(x))\n",
    "        x = self.silu2(self.fc2(x))\n",
    "        x = self.silu3(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "class NN5(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NN5, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "        self.silu1 = nn.SiLU()\n",
    "        self.silu2 = nn.SiLU()\n",
    "        self.silu3 = nn.SiLU()\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "        self.dropout2 = nn.Dropout(p=0.5)\n",
    "        self.dropout3 = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.silu1(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.silu2(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.silu3(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "class NN6(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NN6, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, 16)\n",
    "        self.fc4 = nn.Linear(16, 1)\n",
    "        self.silu1 = nn.SiLU()\n",
    "        self.silu2 = nn.SiLU()\n",
    "        self.silu3 = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.silu1(self.fc1(x))\n",
    "        x = self.silu2(self.fc2(x))\n",
    "        x = self.silu3(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "class NN7(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NN7, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 4)\n",
    "        self.fc4 = nn.Linear(4, 2)\n",
    "        self.fc5 = nn.Linear(2, 1)\n",
    "        self.silu1 = nn.SiLU()\n",
    "        self.silu2 = nn.SiLU()\n",
    "        self.silu3 = nn.SiLU()\n",
    "        self.silu4 = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.silu1(self.fc1(x))\n",
    "        x = self.silu2(self.fc2(x))\n",
    "        x = self.silu3(self.fc3(x))\n",
    "        x = self.silu4(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "    \n",
    "class NN8(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NN8, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.silu1 = nn.SiLU()\n",
    "        self.silu2 = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.silu1(self.fc1(x))\n",
    "        x = self.silu2(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Define model, loss function, and optimizer\n",
    "input_size = len(x_cols)\n",
    "criterion = nn.MSELoss()\n",
    "# criterion = nn.L1Loss()"
   ],
   "id": "d42022d4a047cde7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Neural Network",
   "id": "5db0cbf3ca5dd901"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score, f1_score\n",
    "\n",
    "def scores(y_true, y_pred, mode):\n",
    "    y_true = np.array(y_true, dtype=np.float64)\n",
    "    y_pred = np.array(y_pred, dtype=np.float64)\n",
    "    y_pred[y_pred<THRESHOLD_0] = THRESHOLD_0\n",
    "\n",
    "    y_under_150 = (y_true<THRESHOLD_150) * 1\n",
    "    y_above_150 = (y_true>=THRESHOLD_150) * 1\n",
    "\n",
    "    pred_under_150 = (y_pred<THRESHOLD_150) * 1\n",
    "    pred_above_150 = (y_pred>=THRESHOLD_150) * 1\n",
    "\n",
    "    y_0_pred_0 = int(sum(y_under_150 & pred_under_150))\n",
    "    y_0_pred_1 = int(sum(y_under_150 & pred_above_150))\n",
    "    y_1_pred_0 = int(sum(y_above_150 & pred_under_150))\n",
    "    y_1_pred_1 = int(sum(y_above_150 & pred_above_150))\n",
    "\n",
    "    total = len(y_true)\n",
    "    correct = int(y_0_pred_0) + int(y_1_pred_1)\n",
    "\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_above_150, pred_above_150)\n",
    "\n",
    "    ret = {\n",
    "        mode + ' y_0 pred_0': y_0_pred_0,\n",
    "        mode + ' y_0 pred_1': y_0_pred_1,\n",
    "        mode + ' y_1 pred_0': y_1_pred_0,\n",
    "        mode + ' y_1 pred_1': y_1_pred_1,\n",
    "        mode + ' lazy acc': y_0_pred_0 / (y_0_pred_0 + y_0_pred_1),\n",
    "        mode + ' active acc': y_1_pred_1 / (y_1_pred_0 + y_1_pred_1),\n",
    "        mode + ' r2_score': r2,\n",
    "        mode + ' f1_score': f1,\n",
    "        mode + ' mse': mean_squared_error(y_true, y_pred),\n",
    "        mode + ' acc': correct/total\n",
    "    }\n",
    "\n",
    "    return ret\n",
    "\n",
    "def get_metrics(y_true, y_pred):\n",
    "    y_pred[y_pred<THRESHOLD_0] = THRESHOLD_0\n",
    "\n",
    "    y_under_150 = (y_true<THRESHOLD_150) * 1\n",
    "    y_above_150 = (y_true>=THRESHOLD_150) * 1\n",
    "\n",
    "    pred_under_150 = (y_pred<THRESHOLD_150) * 1\n",
    "    pred_above_150 = (y_pred>=THRESHOLD_150) * 1\n",
    "\n",
    "    y_0_pred_0 = int(sum(y_under_150 & pred_under_150))\n",
    "    y_0_pred_1 = int(sum(y_under_150 & pred_above_150))\n",
    "    y_1_pred_0 = int(sum(y_above_150 & pred_under_150))\n",
    "    y_1_pred_1 = int(sum(y_above_150 & pred_above_150))\n",
    "    \n",
    "    return y_0_pred_0, y_0_pred_1, y_1_pred_0, y_1_pred_1\n",
    "\n",
    "def nn_evaluate(pmodel, data_loader, mode='unassigned'):\n",
    "    y_true = np.array([])\n",
    "    y_pred = np.array([])\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(tqdm(data_loader)):\n",
    "            slabels = labels.reshape((-1,)).numpy()\n",
    "            y_true = np.concatenate((y_true, slabels))\n",
    "            outputs = pmodel(inputs)\n",
    "            spredicted = outputs.reshape((-1,)).numpy()\n",
    "            y_pred = np.concatenate((y_pred, spredicted))\n",
    "\n",
    "    return scores(y_true, y_pred, mode)\n",
    "\n",
    "def nn_train(pmodel, ploader, pcriterion, poptimizer, model_path, num_epochs=10):\n",
    "    ret = []\n",
    "    best_f1 = 0\n",
    "    \n",
    "    scheduler = lr_scheduler.ExponentialLR(poptimizer, gamma=0.5)\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0\n",
    "        print(f\"\\n\\n\")\n",
    "        print(f\"===== ===== ===== ===== ===== \")\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        \n",
    "        print(\"Evaluating\")\n",
    "        test_normal_ret = nn_evaluate(pmodel, test_normal_dataloader, mode='normal')\n",
    "        test_covid_ret = nn_evaluate(pmodel, test_covid_dataloader, mode='covid')\n",
    "        print(f\"epoch-{epoch}, normal_acc: {test_normal_ret['normal acc']:.3f}, normal_f1: {test_normal_ret['normal f1_score']:.3f}, normal_mse: {test_normal_ret['normal mse']:.3f}\")\n",
    "        print(f\"epoch-{epoch}, covid_acc: {test_covid_ret['covid acc']:.3f}, covid_f1: {test_covid_ret['covid f1_score']:.3f}, covid_mse: {test_covid_ret['covid mse']:.3f}\")\n",
    "\n",
    "        temp_ret = {'epoch': epoch}\n",
    "        temp_ret.update(test_normal_ret)\n",
    "        temp_ret.update(test_covid_ret)\n",
    "\n",
    "        ret.append(temp_ret)\n",
    "            \n",
    "        agg_f1 = (test_normal_ret['normal f1_score'] + test_covid_ret['covid f1_score']) / 2\n",
    "        if agg_f1 >= best_f1:\n",
    "            best_f1 = agg_f1\n",
    "            torch.save(pmodel, os.path.join(DIR_PIPELINE, 'store', 'model weight', model_path))\n",
    "        \n",
    "        print(\"Training\")\n",
    "        for i, (inputs, labels) in enumerate(tqdm(ploader)):\n",
    "            # Forward pass            \n",
    "            outputs = pmodel(inputs)\n",
    "            loss = pcriterion(outputs, labels)\n",
    "        \n",
    "            # Backward pass and optimization\n",
    "            poptimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            poptimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Step the scheduler at the end of each epoch\n",
    "        scheduler.step()\n",
    "            \n",
    "    return pmodel, ret"
   ],
   "id": "e8f54733bc0f2deb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model = NN7(input_size)",
   "id": "73aac76aee26bfc1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.0005, momentum=0.9)\n",
    "model, ret = nn_train(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    \"NN7.pth\",\n",
    "    10\n",
    ")"
   ],
   "id": "7a4e0bfa9ae700c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Evaluate",
   "id": "27f96d4cc3003ac9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ret_dta = pd.DataFrame(ret)",
   "id": "b45e8419fc61c4d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the epoch column as the index\n",
    "ret_dta.set_index('epoch', inplace=True)\n",
    "\n",
    "# Plotting the F1 scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=ret_dta[['normal f1', 'covid f1']])\n",
    "plt.title('F1 Score per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend(title='Test Set', labels=['Normal F1', 'COVID F1'])\n",
    "plt.show()"
   ],
   "id": "851625a49812e984",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model_unweighted = torch.load(os.path.join(DIR_PIPELINE, 'store', 'model weight', \"NN7-5.pth\"))",
   "id": "f62e3ae194199983",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_unweighted.eval()\n",
    "# ret1 = nn_evaluate(model_unweighted, train_dataloader, mode='train')\n",
    "ret2 = nn_evaluate(model_unweighted, test_normal_dataloader, mode='normal')\n",
    "ret3 = nn_evaluate(model_unweighted, test_covid_dataloader, mode='covid')\n",
    "# pprint.pprint(ret1)\n",
    "pprint.pprint(ret2)\n",
    "pprint.pprint(ret3)"
   ],
   "id": "56ab77226559e3f8",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
